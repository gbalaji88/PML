---
title: 'Practical Machine Learning Prediction Project'
author: "Balaji"

---


# Prepare the datasets

Load the training data into a data table.

```{r}
echo = TRUE
library(data.table)
library(utils)
url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
D <- fread(url)
```

Load the testing data into a data table.

```{r}
url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
TestSet <- fread(url)
```

Which variables in the test dataset have zero `NA`s?

Belt, arm, dumbbell, and forearm variables that do not have any missing values in the test dataset will be **predictor candidates**.

```{r}
isAnyMissing <- sapply(TestSet, function (x) any(is.na(x) | x == ""))
isPredictor <- !isAnyMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(isAnyMissing))
predCandidates <- names(isAnyMissing)[isPredictor]
predCandidates
```

Subset the primary dataset to include only the **predictor candidates** and the outcome variable, `classe`.

```{r}
varToInclude <- c("classe", predCandidates)
D <- D[, varToInclude, with=FALSE]
dim(D)
names(D)
```

Make `classe` into a factor.

```{r}
D <- D[, classe := factor(D[, classe])]
D[, .N, classe]
```

Split the dataset into a 60% training and 40% probing dataset.

```{r}
library(caret)
seed <- as.numeric(as.Date("2014-10-26"))
set.seed(seed)
inTrain <- createDataPartition(D$classe, p=0.6)
DTrain <- D[inTrain[[1]]]
DCV <- D[-inTrain[[1]]]
```

Preprocess the prediction variables by centering and scaling.

```{r}
X <- DTrain[, predCandidates, with=FALSE]
preProc <- preProcess(X)
preProc
XCS <- predict(preProc, X)
DTrainCS <- data.table(data.frame(classe = DTrain[, classe], XCS))
```

Apply the centering and scaling to the probing dataset.

```{r}
X <- DCV[, predCandidates, with=FALSE]
XCS <- predict(preProc, X)
DCVCS <- data.table(data.frame(classe = DCV[, classe], XCS))
```

Check for near zero variance.

```{r}
nzv <- nearZeroVar(DTrainCS, saveMetrics=TRUE)
if (any(nzv$nzv)) nzv else message("No variables with near zero variance")
```

# Train a prediction model

I chose to use random forests for a prediction model
The error will be estimated using the 40% probing sample.


Fit model over the tuning parameters.

```{r}
#system.time(trainingModel <- train(classe ~ ., data=DTrainCS, method="rf"))
if (file.exists("trainingModel.RData")) {
  load("trainingModel.RData") 
  } else 
    trainingModel <- train(classe ~ ., data=DTrainCS, method="rf")
```


## Evaluate the model on the training dataset

```{r}
trainingModel
phat <- predict(trainingModel, DTrainCS)
confusionMatrix(phat, DTrain[, classe])
```

The training model seems to perform with 100% accuracy. This could either be a really good
model, or we might have over-fit the data. Let us explore using a cross-validation dataset.

## Evaluate the model on the cross-validation dataset

```{r}
phat <- predict(trainingModel, DCVCS)
confusionMatrix(phat, DCVCS[, classe])
```

Good News! The out-of-sample error should hopefully be less than 1%

## Display the final model

```{r finalModel}
varImp(trainingModel)
trainingModel$finalModel
```

We see that the estimated error rate is less than 1%

Save training model object for later.

```{r}
save(trainingModel, file="trainingModel.RData")
```


# Predict on the test data

Load the training model.

```{r}
load(file="trainingModel.RData", verbose=TRUE)
```

Get predictions and evaluate.

```{r}
TestSetCS <- predict(preProc, TestSet[, predCandidates, with=FALSE])
hat <- predict(trainingModel, TestSetCS)
TestSet <- cbind(hat , TestSet)
subset(TestSet, select=names(TestSet)[grep("belt|[^(fore)]arm|dumbbell|forearm", names(TestSet), invert=TRUE)])
```

## Submission to Coursera

Write submission files to `PMLfiles/`.

```{r}
save_files = function(x){
  n = length(x)
  path <- "PMLfiles/"
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=file.path(path, filename),quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }
save_files(hat)
```